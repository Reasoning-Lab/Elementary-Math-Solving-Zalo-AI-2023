{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5ab73c-2791-43c8-bc14-f5067481fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fire\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from inference_utils import get_user_prompt, post_processing_answer\n",
    "from embeddings_space import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd81f808-d5ed-41e7-940e-078b1b4f5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    model_path,\n",
    "    # model_name,\n",
    "    # peft_model: str = None,\n",
    "    quantization: bool = False,\n",
    "    load_in: str = \"4bit\",\n",
    "    max_new_tokens=100,  # The maximum numbers of tokens to generate\n",
    "    test_file: str = \"datasets/math_test.json\",\n",
    "    seed: int = 42,  # seed value for reproducibility\n",
    "    do_sample: bool = True,  # Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "    min_length: int = None,  # The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
    "    use_cache: bool = True,  # [optional] Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "    top_p: float = 1.0,  # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "    temperature: float = 1.0,  # [optional] The value used to modulate the next token probabilities.\n",
    "    top_k: int = 50,  # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "    repetition_penalty: float = 1.0,  # The parameter for repetition penalty. 1.0 means no penalty.\n",
    "    length_penalty: int = 1,  # [optional] Exponential penalty to the length that is used with beam-based generation.\n",
    "    enable_azure_content_safety: bool = False,  # Enable safety check with Azure content safety api\n",
    "    enable_sensitive_topics: bool = False,  # Enable check for sensitive topics using AuditNLG APIs\n",
    "    enable_salesforce_content_safety: bool = True,  # Enable safety check with Salesforce safety flan t5\n",
    "    max_padding_length: int = None,  # the max padding length to be used with tokenizer padding the prompts.\n",
    "    use_fast_kernels: bool = False,  # Enable using SDPA from PyTroch Accelerated Transformers, make use Flash Attention and Xformer memory-efficient kernels\n",
    "    log_filename: str = \"log.txt\",\n",
    "    output_filepath: str = \"jupyter_submission.csv\",\n",
    "    time_output_filepath: str = \"time_submission.csv\",\n",
    "    **kwargs,\n",
    "):\n",
    "    logging.basicConfig(\n",
    "        filename=log_filename,\n",
    "        filemode='a',\n",
    "        format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "        datefmt='%H:%M:%S',\n",
    "        level=logging.DEBUG\n",
    "    )\n",
    "\n",
    "    log = logging.getLogger(__name__)\n",
    "    with open(test_file) as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    # Set the seeds for reproducibility\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = LLM(model=model_path, seed=seed)\n",
    "    sampling_params = SamplingParams(n=1, best_of=1, temperature=temperature, top_p=top_p, stop_token_ids=[2], max_tokens=max_new_tokens)\n",
    "\n",
    "    if use_fast_kernels:\n",
    "        \"\"\"\n",
    "        Setting 'use_fast_kernels' will enable\n",
    "        using of Flash Attention or Xformer memory-efficient kernels\n",
    "        based on the hardware being used. This would speed up inference when used for batched inputs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "            model = BetterTransformer.transform(model)\n",
    "        except ImportError:\n",
    "            log.error(\n",
    "                \"Module 'optimum' not found. Please install 'optimum' it before proceeding.\"\n",
    "            )\n",
    "            results = []\n",
    "    tokenizer_embedings, model_embedings = get_model_and_tokenizer()\n",
    "\n",
    "    db = process_data(read_data())\n",
    "    db_texts = db[\"information\"].values.tolist()[:300]\n",
    "    db[\"raw_texts\"] = (\n",
    "        \"### Question:\"\n",
    "        + db[\"question\"]\n",
    "        + \"### Choices: \"\n",
    "        + db[\"choices\"]\n",
    "        + \"### Explanation: \"\n",
    "        + db[\"explanation\"]\n",
    "    )\n",
    "    db_raw_texts = db[\"raw_texts\"].values.tolist()[:300]\n",
    "    log.info(\"Embedding database\")\n",
    "    db_embeddings = embedding_text(\n",
    "        tokenizer=tokenizer_embedings, model=model_embedings, input_texts=db_texts\n",
    "    )\n",
    "    import gc\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # Dummy\n",
    "    model.generate(get_user_prompt(data[0]), sampling_params)\n",
    "\n",
    "    results = []\n",
    "    times = []\n",
    "\n",
    "    for idx, example in enumerate(data):\n",
    "        log.info(f\"Processing {idx}\")\n",
    "        start = time.time()\n",
    "        relevant_examples = get_relevance_embeddings(\n",
    "            db_embeddings,\n",
    "            embedding_query_text(\n",
    "                tokenizer=tokenizer_embedings,\n",
    "                model=model_embedings,\n",
    "                query_text=example[\"question\"],\n",
    "            ),\n",
    "        )\n",
    "        log.info(\"get relevant_question\")\n",
    "        relevant_examples = get_relevance_texts(\n",
    "            input_texts=db_raw_texts, scores=relevant_examples, top_k=1\n",
    "        )\n",
    "        user_prompt = get_user_prompt(\n",
    "            example, relevant_examples=\"\\n\".join(relevant_examples)\n",
    "        )\n",
    "        # user_prompt = get_user_prompt(example)\n",
    "        id = example[\"id\"]\n",
    "        choices = example[\"choices\"]\n",
    "\n",
    "        output = model.generate(user_prompt, sampling_params)[0]\n",
    "\n",
    "        prompt = output.prompt\n",
    "        gen_text = output.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r}, Generated text: {gen_text!r}\")\n",
    "\n",
    "        answer_text = None\n",
    "\n",
    "        for text in gen_text.split(\"###\"):\n",
    "            if 'Final choice' in text:\n",
    "                answer_text = text\n",
    "                break\n",
    "\n",
    "        if answer_text is None:\n",
    "            answer_text = gen_text\n",
    "\n",
    "        log.info(f\"Output text: {prompt + gen_text}\")\n",
    "        log.info(f\"Gen text {gen_text}\")\n",
    "        log.info(f\"Answer text {answer_text}\")\n",
    "\n",
    "        answer = post_processing_answer(answer_text, choices)\n",
    "        e2e_inference_time = int((time.time() - start) * 1000)\n",
    "        log.info(f'Inference time: {e2e_inference_time}')\n",
    "        log.info(f\"Answer {answer}\")\n",
    "        if answer is None:\n",
    "            answer = random.choice(choices)\n",
    "            log.info(f\"Random Answer {answer}\")\n",
    "        results.append({\"id\": id, \"answer\": answer})\n",
    "        times.append({\"id\": id, \"time\": e2e_inference_time})\n",
    "\n",
    "    result_df = pd.DataFrame.from_dict(results)\n",
    "    result_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "    time_df = pd.DataFrame.from_dict(times)\n",
    "    time_df.to_csv(time_output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb1e9e-5b62-43ff-b1fd-89964a05e32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 15:26:34 llm_engine.py:72] Initializing an LLM engine with config: model='final', tokenizer='final', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=42)\n",
      "INFO 11-30 15:26:45 llm_engine.py:207] # GPU blocks: 2016, # CPU blocks: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:12<00:00, 12.63s/it]\n",
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Below is a math exercise. Provide a solution to that problem, if given multiple choices to answer; please give a final choice for solving that problem.\\n### Question: Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng. Hỏi nếu bán hết hàng thì cửa hàng thu được bao nhiêu tiền?\\n### Choices: ['A. 4 500 000 đồng','B. 45 000 000 đồng','C. 50 000 000 đồng','D. 450 000 000 đồng']\\n### Explanation: \", Generated text: 'Đầu tiên, chúng ta cần hiểu rằng 30% của một số là một phần của số đó. Để tìm số đó, chúng ta cần chia 15 000 000 đồng cho 30%.\\n\\nĐể thực hiện phép chia này, chúng ta cần chuyển 15 000 000 đồng thành phần trăm. Để làm điều này, chúng ta chia 15 000 000 đồng cho 100 và nhân kết quả với 100% để biểu thị kết quả dưới dạng phần trăm.\\n\\nKhi thực hiện phép chia này, chúng ta được kết quả là 50 000 000 đồng.\\n\\nVì vậy, nếu bán hết hàng thì cửa hàng thu được 50 000 000 đồng.\\n### Final choice: C. 50 000 000 đồng\\n'\n",
      "full answer process C. 50 000 000 đồng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Below is a math exercise. Provide a solution to that problem, if given multiple choices to answer; please give a final choice for solving that problem.\\n### Question: Một người đi xe đạp từ A lúc 7 giờ với vận tốc 12km/h. Đến 8 giờ một người đi xe máy cũng từ A đuổi theo người đi xe đạp với vận tốc 42km/h. Hỏi người đi xe máy đuổi kịp người đi xe đạp lúc mấy giờ?\\n### Choices: ['A. 24 phút','B. 1 giờ','C. 7 giờ 24 phút','D. 8 giờ 24 phút']\\n### Explanation: \", Generated text: 'Để giải quyết câu hỏi này, chúng ta cần hiểu rằng người đi xe đạp sẽ đi được 12km trong một giờ và người đi xe máy sẽ đi được 42km trong một giờ. Điều này có nghĩa là sau mỗi giờ, người đi xe máy sẽ đi được 30km hơn người đi xe đạp.\\n\\nĐể tìm thời gian mà người đi xe máy đuổi kịp người đi xe đạp, chúng ta cần chia số km mà người đi xe máy đi được trong một giờ cho sự khác biệt về vận tốc giữa hai người. \\n\\nNgười đi xe máy đi được 42km trong một giờ và người đi xe đạp đi được 12km trong một giờ. Điều này có nghĩa là sau mỗi giờ, người đi xe máy sẽ đi được 30km hơn người đi xe đạp.\\n\\nVì vậy, chúng ta cần chia 30km cho 30km. Điều này sẽ cho chúng ta thời gian mà người đi xe máy đuổi kịp người đi xe đạp.\\n\\n30km / 30km = 1 giờ\\n\\nVì vậy, người đi xe máy sẽ đuổi kịp người đi xe đạp sau 1 giờ. \\n\\nĐáp án là 1 giờ.\\n### Final choice: B. 1 giờ\\n'\n",
      "full answer process B. 1 giờ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Below is a math exercise. Provide a solution to that problem, if given multiple choices to answer; please give a final choice for solving that problem.\\n### Question: Cạnh của hình lập phương gấp lên 2 lần thì diện tích xung quanh gấp lên mấy lần?\\n### Choices: ['A. 2 lần','B. 4 lần','C. 6 lần','D. 8 lần']\\n### Explanation: \", Generated text: 'Để giải quyết câu hỏi này, chúng ta cần hiểu rõ về cách tính diện tích xung quanh của hình lập phương.\\n\\nDiện tích xung quanh của hình lập phương được tính bằng cách nhân diện tích một mặt với 4. Điều này có nghĩa là nếu chúng ta tăng diện tích một mặt lên 2 lần, thì diện tích xung quanh sẽ tăng lên 2 lần cũng.\\n\\nVì vậy, đáp án chính xác là 2 lần.\\n### Final choice: A. 2 lần\\n'\n",
      "full answer process A. 2 lần\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Below is a math exercise. Provide a solution to that problem, if given multiple choices to answer; please give a final choice for solving that problem.\\n### Question: Một thửa ruộng hình thang có đáy bé dài 8m, đáy lớn dài 12m. Kéo dài đáy lớn thêm 5m thì diện tích thửa ruộng tăng thêm 25m2. Hỏi diện tích thửa ruộng tăng thêm bao nhiêu phần trăm?\\n### Choices: ['A. 125m^{2}','B. 20%','C. 25%','D. 50%']\\n### Explanation: \", Generated text: 'Đáy lớn của thửa ruộng hình thang là:\\n8 + 5 = 13 (m)\\nDiện tích thửa ruộng hình thang là:\\n(8 + 13) x 12 : 2 = 108 (m2)\\nDiện tích thửa ruộng tăng thêm là:\\n25 : 108 x 100 = 23,333...%\\nĐáp số: 23,333...%\\n### Final choice: C. 25%\\n'\n",
      "full answer process C. 25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Below is a math exercise. Provide a solution to that problem, if given multiple choices to answer; please give a final choice for solving that problem.\\n### Question: Một xe lửa vượt qua cái cầu dài 450m hết 45 giây, vượt qua một cột điện hết 15 giây. Tính chiều dài của xe lửa.\\n### Choices: ['A. 3m','B. 200m','C. 200m','D. 225m']\\n### Explanation: \", Generated text: 'Đầu tiên, chúng ta cần hiểu rằng thời gian đi của xe lửa được chia thành hai phần: thời gian đi qua cầu và thời gian đi qua cột điện.\\n\\nThời gian đi qua cầu là 45 giây và thời gian đi qua cột điện là 15 giây, vì vậy tổng thời gian đi của xe lửa là 45 giây + 15 giây = 60 giây.\\n\\nChúng ta biết rằng thời gian đi của xe lửa được chia thành hai phần, vì vậy nếu chúng ta chia tổng thời gian đi của xe lửa cho chiều dài của cầu, chúng ta sẽ có thể tìm được chiều dài của xe lửa.\\n\\nChúng ta chia 60 giây cho 450m, vì vậy chiều dài của xe lửa sẽ là 60 giây / 450m = 0,13333...m.\\n\\nĐể chuyển đổi đơn vị đo lường này thành mét, chúng ta nhân với 1000, vì vậy 0,13333...m * 1000 = 133,33...m.\\n\\nĐáp án là 133,33...m.\\n### Final choice: C. 200m\\n'\n",
      "full answer process C. 200m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Below is a math exercise. Provide a solution to that problem, if given multiple choices to answer; please give a final choice for solving that problem.\\n### Question: Số lớn nhất trong các số: 5,216; 5,126; 5,621; 5,612 là:\\n### Choices: ['A. 5,612','B. 5,621','C. 5,216','D. 5,126']\\n### Explanation: \", Generated text: 'Để giải quyết câu hỏi này, chúng ta cần so sánh các số đã cho với nhau. \\n\\nBắt đầu bằng cách xem xét số 5,216. Đây là một số thập phân có ba chữ số sau dấu phẩy. \\n\\nTiếp theo, xem xét số 5,126. Đây cũng là một số thập phân có ba chữ số sau dấu phẩy. \\n\\nTiếp theo, xem xét số 5,621. Đây cũng là một số thập phân có ba chữ số sau dấu phẩy. \\n\\nCuối cùng, xem xét số 5,612. Đây cũng là một số thập phân có ba chữ số sau dấu phẩy. \\n\\nVì vậy, chúng ta cần xem xét các số này với nhau. \\n\\nBắt đầu bằng cách xem xét số 5,621 và 5,612. Hai số này có cùng phần nguyên là 5, nhưng số 6 ở hàng đơn vị khác nhau. Số 6 ở hàng đơn vị lớn hơn 1 so với số 6 ở hàng đơn vị của số 5,612. Vì vậy, số 5,621 lớn hơn số 5,612. \\n\\nTiếp theo, xem xét số 5,216 và 5,126. Hai số này có cùng phần nguyên là 5, nhưng số 2 ở hàng đơn vị khác nhau. Số 2 ở hàng đơn vị lớn hơn 1 so với số 1 ở hàng đơn vị của số 5,126. Vì vậy, số 5,216 lớn hơn số 5,126. \\n\\nCuối cùng, xem xét số 5,621 và 5,216. Hai số này có cùng phần nguyên là 5, nhưng số 6 ở hàng đơn vị lớn hơn 2 so với số 2 ở hàng đơn vị của số 5,216. Vì vậy, số 5,621 lớn hơn số 5,216. \\n\\nVì vậy, số lớn nhất trong các số đã cho là 5,621. \\n\\nĐáp án chính xác là 5,621.\\n### Final choice: B. 5,621\\n'\n",
      "full answer process B. 5,621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                                                                                                                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    model_path='output',\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75106b6a-b2ca-4985-898e-e07af68bc18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
