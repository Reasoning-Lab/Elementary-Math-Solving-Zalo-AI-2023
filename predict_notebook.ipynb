{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5ab73c-2791-43c8-bc14-f5067481fd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-06 14:49:53,710\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-12-06 14:49:53,780\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import fire\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from inference_utils import get_user_prompt, post_processing_answer\n",
    "from embeddings_space import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd81f808-d5ed-41e7-940e-078b1b4f5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    model_path,\n",
    "    # model_name,\n",
    "    # peft_model: str = None,\n",
    "    quantization: bool = False,\n",
    "    load_in: str = \"4bit\",\n",
    "    max_new_tokens=100,  # The maximum numbers of tokens to generate\n",
    "    test_file: str = \"datasets/math_test.json\",\n",
    "    seed: int = 42,  # seed value for reproducibility\n",
    "    do_sample: bool = True,  # Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "    min_length: int = None,  # The minimum length of the sequence to be generated, input prompt + min_new_tokens\n",
    "    use_cache: bool = True,  # [optional] Whether or not the model should use the past last key/values attentions Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.\n",
    "    top_p: float = 1.0,  # [optional] If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "    temperature: float = 1.0,  # [optional] The value used to modulate the next token probabilities.\n",
    "    top_k: int = 50,  # [optional] The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "    repetition_penalty: float = 1.0,  # The parameter for repetition penalty. 1.0 means no penalty.\n",
    "    length_penalty: int = 1,  # [optional] Exponential penalty to the length that is used with beam-based generation.\n",
    "    enable_azure_content_safety: bool = False,  # Enable safety check with Azure content safety api\n",
    "    enable_sensitive_topics: bool = False,  # Enable check for sensitive topics using AuditNLG APIs\n",
    "    enable_salesforce_content_safety: bool = True,  # Enable safety check with Salesforce safety flan t5\n",
    "    max_padding_length: int = None,  # the max padding length to be used with tokenizer padding the prompts.\n",
    "    use_fast_kernels: bool = False,  # Enable using SDPA from PyTroch Accelerated Transformers, make use Flash Attention and Xformer memory-efficient kernels\n",
    "    log_filename: str = \"log.txt\",\n",
    "    output_filepath: str = \"jupyter_submission.csv\",\n",
    "    time_output_filepath: str = \"time_submission.csv\",\n",
    "    **kwargs,\n",
    "):\n",
    "    logging.basicConfig(\n",
    "        filename=log_filename,\n",
    "        filemode='a',\n",
    "        format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "        datefmt='%H:%M:%S',\n",
    "        level=logging.DEBUG\n",
    "    )\n",
    "\n",
    "    log = logging.getLogger(__name__)\n",
    "    with open(test_file) as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "\n",
    "    # Set the seeds for reproducibility\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    model = LLM(model=model_path, seed=seed)\n",
    "    sampling_params = SamplingParams(n=1, best_of=1, temperature=temperature, top_p=top_p, stop_token_ids=[2], max_tokens=max_new_tokens)\n",
    "\n",
    "    if use_fast_kernels:\n",
    "        \"\"\"\n",
    "        Setting 'use_fast_kernels' will enable\n",
    "        using of Flash Attention or Xformer memory-efficient kernels\n",
    "        based on the hardware being used. This would speed up inference when used for batched inputs.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "            model = BetterTransformer.transform(model)\n",
    "        except ImportError:\n",
    "            log.error(\n",
    "                \"Module 'optimum' not found. Please install 'optimum' it before proceeding.\"\n",
    "            )\n",
    "            results = []\n",
    "    tokenizer_embedings, model_embedings = get_model_and_tokenizer()\n",
    "    model_embedings.cuda()\n",
    "\n",
    "    db = process_data(read_data())\n",
    "    db_texts = db[\"information\"].values.tolist()[:500]\n",
    "    db[\"raw_texts\"] = (\n",
    "        \"### Question:\"\n",
    "        + db[\"question\"]\n",
    "        + \"### Choices: \"\n",
    "        + db[\"choices\"]\n",
    "        + \"### Explanation: \"\n",
    "        + db[\"explanation\"]\n",
    "    )\n",
    "    db_raw_texts = db[\"raw_texts\"].values.tolist()[:500]\n",
    "    log.info(\"Embedding database\")\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    db_embeddings = embedding_text(\n",
    "        tokenizer=tokenizer_embedings, model=model_embedings, input_texts=db_texts\n",
    "    )\n",
    "    print(db_embeddings.shape)\n",
    "    # return None\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    # Dummy\n",
    "    model.generate(get_user_prompt(data[0]), sampling_params)\n",
    "\n",
    "    results = []\n",
    "    times = []\n",
    "\n",
    "    for idx, example in enumerate(data):\n",
    "        log.info(f\"Processing {idx}\")\n",
    "        start = time.time()\n",
    "        relevant_examples = get_relevance_embeddings(\n",
    "            db_embeddings,\n",
    "            embedding_query_text(\n",
    "                tokenizer=tokenizer_embedings,\n",
    "                model=model_embedings,\n",
    "                query_text=example[\"question\"],\n",
    "            ),\n",
    "        )\n",
    "        log.info(\"get relevant_question\")\n",
    "        relevant_examples = get_relevance_texts(\n",
    "            input_texts=db_raw_texts, scores=relevant_examples, top_k=1\n",
    "        )\n",
    "        user_prompt = get_user_prompt(\n",
    "            example, relevant_examples=\"\\n\".join(relevant_examples)\n",
    "        )\n",
    "        # user_prompt = get_user_prompt(example)\n",
    "        id = example[\"id\"]\n",
    "        choices = example[\"choices\"]\n",
    "\n",
    "        output = model.generate(user_prompt, sampling_params)[0]\n",
    "\n",
    "        prompt = output.prompt\n",
    "        gen_text = output.outputs[0].text\n",
    "        print(f\"Prompt: {prompt!r}, Generated text: {gen_text!r}\")\n",
    "\n",
    "        answer_text = None\n",
    "\n",
    "        for text in gen_text.split(\"###\"):\n",
    "            if 'Final choice' in text:\n",
    "                answer_text = text\n",
    "                break\n",
    "\n",
    "        if answer_text is None:\n",
    "            answer_text = gen_text\n",
    "\n",
    "        log.info(f\"Output text: {prompt + gen_text}\")\n",
    "        log.info(f\"Gen text {gen_text}\")\n",
    "        log.info(f\"Answer text {answer_text}\")\n",
    "\n",
    "        answer = post_processing_answer(answer_text, choices)\n",
    "        e2e_inference_time = int((time.time() - start) * 1000)\n",
    "        log.info(f'Inference time: {e2e_inference_time}')\n",
    "        log.info(f\"Answer {answer}\")\n",
    "        if answer is None:\n",
    "            answer = random.choice(choices)\n",
    "            log.info(f\"Random Answer {answer}\")\n",
    "        results.append({\"id\": id, \"answer\": answer})\n",
    "        times.append({\"id\": id, \"time\": e2e_inference_time})\n",
    "\n",
    "    result_df = pd.DataFrame.from_dict(results)\n",
    "    result_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "    time_df = pd.DataFrame.from_dict(times)\n",
    "    time_df.to_csv(time_output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fb1e9e-5b62-43ff-b1fd-89964a05e32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 14:49:53 llm_engine.py:73] Initializing an LLM engine with config: model='/output', tokenizer='/output', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=42)\n",
      "INFO 12-06 14:50:07 llm_engine.py:222] # GPU blocks: 2146, # CPU blocks: 2048\n",
      "(500, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:12<00:00, 12.01s/it]\n",
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:16<00:00, 16.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Below is a math exercise. Provide a solution to that problem, if given multiple choices to answer; please give a final choice for solving that problem.\\nSimilar Question and their explanation: ### Question:Bạn Nam mang hai tờ tiền có mệnh giá 10 000 đồng đi mua bút chì. Bạn mua hết 15 000. Bạn Nam còn thừa ...………đồng.### Choices: ['A. 10 000 đồng', 'B. 5 000 đồng', 'C. 2 000 đồng', 'D. 1 000 đồng']### Explanation: Đầu tiên, chúng ta cần hiểu rõ về câu hỏi. Bạn Nam mang theo hai tờ tiền mỗi tờ có mệnh giá 10 000 đồng, tức là tổng cộng bạn Nam có 10 000 đồng x 2 = 20 000 đồng.\\n\\nSau đó, bạn Nam đã dùng 15 000 đồng để mua bút chì. Vậy, số tiền bạn Nam đã dùng là 15 000 đồng.\\n\\nCuối cùng, để tìm ra số tiền bạn Nam còn lại sau khi mua bút chì, chúng ta lấy tổng số tiền bạn Nam có ban đầu trừ đi số tiền bạn Nam đã dùng để mua bút chì. Tức là: 20 000 đồng - 15 000 đồng = 5 000 đồng.\\n\\nVậy, bạn Nam còn thừa 5 000 đồng sau khi mua bút chì. Đáp án chính xác là 5 000 đồng.\\n### Question: Một cửa hàng đã bán 30% số hàng hiện có và thu được 15 000 000 đồng. Hỏi nếu bán hết hàng thì cửa hàng thu được bao nhiêu tiền?\\n### Choices: ['A. 4 500 000 đồng','B. 45 000 000 đồng','C. 50 000 000 đồng','D. 450 000 000 đồng']\\n### Explanation: \", Generated text: 'Để giải quyết câu hỏi này, chúng ta cần hiểu rằng 30% của một số là một phần của số đó. Điều này có nghĩa là nếu chúng ta có 100 phần của một số, thì 30% số đó sẽ là 30 phần.\\n\\nVì vậy, để tìm ra số hàng mà cửa hàng có, chúng ta cần chia số hàng đã bán cho 30% của số hàng đó. \\n\\nĐầu tiên, chúng ta cần tìm ra số hàng mà cửa hàng có. Để làm điều này, chúng ta chia số hàng đã bán cho 30% của số hàng đó. \\n\\nVì số hàng đã bán là 15 000 000 đồng và 30% của số hàng đó là 30 phần, nên số hàng mà cửa hàng có là 15 000 000 đồng / 30 phần = 500 000 đồng / phần.\\n\\nTiếp theo, chúng ta cần tìm ra số phần của số hàng mà cửa hàng có. Để làm điều này, chúng ta chia số hàng đã bán cho số hàng mà cửa hàng có. \\n\\nVì số hàng đã bán là 15 000 000 đồng và số hàng mà cửa hàng có là 500 000 đồng / phần, nên số phần của số hàng mà cửa hàng có là 15 000 000 đồng / 500 000 đồng / phần = 30 phần.\\n\\nCuối cùng, chúng ta cần nhân số phần của số hàng mà cửa hàng có với số hàng mà cửa hàng có để tìm ra tổng số hàng. \\n\\nVì số phần của số hàng mà cửa hàng có là 30 và số hàng mà cửa hàng có là 500 000 đồng / phần, nên tổng số hàng là 30 phần * 500 000 đồng / phần = 15 000 000 đồng.\\n\\nVì vậy, đáp án chính xác là 15 000 000 đồng.\\n### Final choice: B. 45 000 000 đồng\\n'\n",
      "full answer process B. 45 000 000 đồng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:18<00:00, 18.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Below is a math exercise. Provide a solution to that problem, if given multiple choices to answer; please give a final choice for solving that problem.\\nSimilar Question and their explanation: ### Question:Cả ngày xe đạp A đi được 7676m và xe đạp B đi được 8km. Hỏi quãng đường của xe đạp A hay xe đạp B đi được dài hơn?### Choices: ['A. Xe A', 'B. Xe B', 'C. Cả hai xe đi quãng đường bằng nhau']### Explanation: Để trả lời câu hỏi này, trước hết chúng ta cần chuyển đổi tất cả các đơn vị đo lường về cùng một đơn vị để so sánh. Trong trường hợp này, chúng ta sẽ chuyển đổi tất cả về mét.\\n\\nXe đạp A đã đi được 7676 mét. Đối với xe đạp B, chúng ta cần chuyển đổi từ km sang mét. Chúng ta biết rằng 1 km = 1000 mét, vì vậy 8 km = 8 * 1000 = 8000 mét.\\n\\nBây giờ chúng ta có thể so sánh: 7676 mét (xe A) so với 8000 mét (xe B). Rõ ràng, 8000 mét lớn hơn 7676 mét.\\n\\nVì vậy, xe đạp B đã đi được quãng đường dài hơn.\\n### Question: Một người đi xe đạp từ A lúc 7 giờ với vận tốc 12km/h. Đến 8 giờ một người đi xe máy cũng từ A đuổi theo người đi xe đạp với vận tốc 42km/h. Hỏi người đi xe máy đuổi kịp người đi xe đạp lúc mấy giờ?\\n### Choices: ['A. 24 phút','B. 1 giờ','C. 7 giờ 24 phút','D. 8 giờ 24 phút']\\n### Explanation: \", Generated text: 'Đầu tiên, chúng ta cần xác định khoảng cách mà người đi xe đạp đã đi được sau mỗi giờ. Để làm điều này, chúng ta chia khoảng cách mà người đi xe đạp đã đi được (7 giờ với vận tốc 12km/h) cho thời gian mà người đi xe đạp đã đi (7 giờ). Kết quả là 12km/h.\\n\\nTiếp theo, chúng ta cần xác định khoảng cách mà người đi xe máy đã đi được sau mỗi giờ. Để làm điều này, chúng ta chia khoảng cách mà người đi xe máy đã đi được (8 giờ với vận tốc 42km/h) cho thời gian mà người đi xe máy đã đi (8 giờ). Kết quả là 5,25km/h.\\n\\nCuối cùng, chúng ta cần xác định khoảng cách mà người đi xe máy cần đi để đuổi kịp người đi xe đạp. Để làm điều này, chúng ta lấy khoảng cách mà người đi xe đạp đã đi được (7 giờ với vận tốc 12km/h) trừ đi khoảng cách mà người đi xe máy đã đi được (8 giờ với vận tốc 42km/h). Kết quả là 18km.\\n\\nVì vậy, người đi xe máy sẽ đuổi kịp người đi xe đạp sau 18km / 5,25km/h = 3,4 giờ.\\n\\nĐể chuyển đổi thời gian này thành phút, chúng ta nhân thời gian đó với 60 (vì 1 giờ bằng 60 phút). Kết quả là 3,4 giờ * 60 phút/giờ = 204 phút.\\n\\nVì vậy, người đi xe máy sẽ đuổi kịp người đi xe đạp sau 204 phút. Để chuyển đổi thành giờ, chúng ta chia thời gian đó cho 60 (vì 1 giờ bằng 60 phút). Kết quả là 204 phút / 60 phút/giờ = 3,4 giờ.\\n\\nVì vậy, người đi xe máy sẽ đuổi kịp người đi xe đạp lúc 11 giờ (8 giờ cộng thêm 3 giờ).\\n\\nVì vậy, đáp án là 11 giờ.\\n### Final choice: B. 1 giờ\\n'\n",
      "full answer process B. 1 giờ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"Below is a math exercise. Provide a solution to that problem, if given multiple choices to answer; please give a final choice for solving that problem.\\nSimilar Question and their explanation: ### Question:Cạnh của hình lập phương gấp lên 2 lần thì thể tích được gấp lên:### Choices: ['A. 2 lần', 'B. 4 lần', 'C. 6 lần', 'D. 8 lần']### Explanation: Cạnh của hình lập phương gấp lên 2 lần thì thể tích được gấp lên: 2 ${\\\\times}$ 2 ${\\\\times}$ 2 = 8 (lần).\\n### Question: Cạnh của hình lập phương gấp lên 2 lần thì diện tích xung quanh gấp lên mấy lần?\\n### Choices: ['A. 2 lần','B. 4 lần','C. 6 lần','D. 8 lần']\\n### Explanation: \", Generated text: 'Để giải thích câu trả lời này, chúng ta cần hiểu rõ về cách tính diện tích xung quanh của hình lập phương.\\n\\nDiện tích xung quanh của hình lập phương được tính bằng cách cộng diện tích của 4 mặt của hình lập phương lại. Mỗi mặt của hình lập phương là một hình vuông có cạnh bằng cạnh của hình lập phương.\\n\\nVì vậy, nếu cạnh của hình lập phương tăng lên 2 lần, thì diện tích của mỗi mặt của hình lập phương sẽ tăng lên 2 lần. Do đó, diện tích xung quanh của hình lập phương sẽ tăng lên 2 lần.\\n\\nVì vậy, câu trả lời cho câu hỏi này là \"2 lần\".\\n### Final choice: A. 2 lần\\n'\n",
      "full answer process A. 2 lần\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Below is a math exercise. Provide a solution to that problem, if given multiple choices to answer; please give a final choice for solving that problem.\\nSimilar Question and their explanation: ### Question:“Một trăm hai mươi lăm mét vuông” được viết là:### Choices: [\\'A. 125m2\\', \\'B. 125m\\', \\'C. 152m\\', \\'D. 152m2\\']### Explanation: Đáp án đúng là 125m2. \\n\\nGiải thích: \\n\\nTrước hết, ta cần hiểu rằng \"một trăm hai mươi lăm\" trong tiếng Việt tương ứng với số 125 trong hệ thống số. \\n\\nTiếp theo, \"mét vuông\" là đơn vị đo diện tích. Trong toán học và vật lý, mét vuông (ký hiệu là m2) được sử dụng để đo diện tích của một hình vuông có cạnh dài 1 mét. \\n\\nVì vậy, khi kết hợp số 125 với đơn vị đo mét vuông, ta được 125m2. \\n\\nCác lựa chọn khác không đúng vì: \\n- 125m là đơn vị đo chiều dài, không phải diện tích.\\n- 152m và 152m2 đều không đúng vì số được viết không phải là 125.\\n### Question: Một thửa ruộng hình thang có đáy bé dài 8m, đáy lớn dài 12m. Kéo dài đáy lớn thêm 5m thì diện tích thửa ruộng tăng thêm 25m2. Hỏi diện tích thửa ruộng tăng thêm bao nhiêu phần trăm?\\n### Choices: [\\'A. 125m^{2}\\',\\'B. 20%\\',\\'C. 25%\\',\\'D. 50%\\']\\n### Explanation: ', Generated text: 'Đầu tiên, ta cần tính diện tích thửa ruộng ban đầu. Để làm điều này, ta áp dụng công thức tính diện tích hình thang: S = (a + b) * h / 2, trong đó a và b là độ dài hai đáy, h là chiều cao.\\n\\nTrong trường hợp này, ta biết rằng a = 8m, b = 12m và h = 5m. Thay số liệu này vào công thức, ta có:\\n\\nS = (8 + 12) * 5 / 2 = 100m2\\n\\nTiếp theo, ta cần tính diện tích thửa ruộng sau khi kéo dài đáy lớn thêm 5m. Để làm điều này, ta áp dụng cùng công thức tính diện tích hình thang nhưng thay số liệu vào:\\n\\nS = (8 + 17) * 5 / 2 = 125m2\\n\\nCuối cùng, ta chia diện tích thửa ruộng sau khi kéo dài đáy lớn thêm 5m cho diện tích thửa ruộng ban đầu và nhân với 100% để biểu thị kết quả dưới dạng phần trăm.\\n\\n125m2 / 100m2 * 100% = 25%\\n\\nVậy, diện tích thửa ruộng tăng thêm 25% so với diện tích ban đầu. Đáp án chính xác là 25%.\\n### Final choice: C. 25%\\n'\n",
      "full answer process C. 25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|                                                                       | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    model_path='output',\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75106b6a-b2ca-4985-898e-e07af68bc18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
