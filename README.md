# ZAIC-2023-Elementary-Math-Solving

# Specs - Cấu hình

Pytorch: 2.1.0
CUDA: 12.1

# Installation - Cài đặt

```bash
pip install -r requirements.txt
```

```bash
huggingface-cli login
wandb login
```

# Data Source
| Source Data File                                | Note                                           |
|-------------------------------------------------|------------------------------------------------|
| Original Train: 1200                           | File Name: math_train.json, with explanation: 537                      |
|                                                 |                                                |
| GPT Generate Explanation:                       |                                                |
| Using GPT-3.5 Turbo:                           | File Name: with-missing-explain-3.5.json      |
| Using GPT-4:                                   | File Name: with-missing-explain-4.json        |
|                                                 |                                                |
| Public Test: 189                               | File Name: math_test.json                      |
| Public Test with Hand Label for Local Evaluation |                                                |
|                                                 |                                                |
| Crawled: 140                                   | Source: ?                                       |
|                                                 |                                                |
| Data for Finetuning: qualified_data.json              |Combination of Cleaned Original Train  Missing Explanation with Generated by GPT-4 |

# Solution overview

## Training

<img src="./figures/Training_ZaloAI_Math_Solving.drawio.png" width=500 heigh=300>

## Inference

Using [intfloat/e5-base](https://huggingface.co/intfloat/e5-base) as embedding model

<img src="./figures/Inference_ZaloAI_Math_Solving.drawio.png" width=500 heigh=300>

# Training - Huấn luyện

## Baseline Llama-2-7b LoRA 8bit

```bash
python llama_recipes/finetuning.py --use_peft --peft_method lora --quantization --model_name meta-llama/Llama-2-7b-hf --output_dir outputs
```

## Finetuning with llama_recipes (deprecated - not using in final solution)

model baseline: zephyr-7b-alpha
with zalo_math_fill_missing_explain_4 (using GPT4)

now with `load_in` options `['4bit', '8bit']`

```bash
python llama_recipes/finetuning.py --use_peft --peft_method lora --quantization --model_name HuggingFaceH4/zephyr-7b-alpha --dataset zalo_math_fill_missing_explain_35 --output_dir outputs --use_wandb --wandb_entity baolocpham --wandb_key KEY --num_epochs 2
```

```bash
python llama_recipes/finetuning.py --use_peft --peft_method lora --quantization --model_name HuggingFaceH4/zephyr-7b-alpha --dataset zalo_math_fill_missing_explain_4 --output_dir outputs --max_length 2048 --num_epochs 6 --load_in 4bit --use_wandb --wandb_entity baolocpham --wandb_key KEY
```

## Pretraining

```bash
bash run_pt.sh
```

## Finetune - SFTTrainer

using [HuggingFaceH4/zephyr-7b-beta](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) as base model

```bash
ACCELERATE_LOG_LEVEL=info accelerate launch --config_file <multi_gpu.yaml / deepspeed_zero3.yaml> --num_processes=1 sft.py config_lora.yaml
```

Ex:

```bash
ACCELERATE_LOG_LEVEL=info accelerate launch --config_file multi_gpu.yaml --num_processes=1 sft.py config_lora.yaml
```

# Inference

## Quantization inference 4bit / 8bit
```bash
python inference.py --model_name hllj/zephyr-7b-beta-vi-math --peft_model outputs-sft-zephyr-beta-v1/checkpoint-1500/ --load_in 4bit/8bit --max_new_tokens 512 --temperature 0.1
```

- model_name: base model using for finetuning
- peft_model: folder contains LoRA finetune output
- load_in: 4bit / 8bit quantization
- max_new_tokens: maximum generating tokens
- temperature: temperature for sampling (we're chosing range from 0.1 to 0.5)

# Inference with vLLM

## Merge base model with LoRA

Because when inference with vLLM, it doesn't allow using LoRA outputs but the merged weights itself

```bash
python merge_peft_adapter.py --model_type auto --base_model <name or path base model> --tokenizer_path <name or path tokenizer> --lora_model <lora folder> --output_dir <output folder for merged model>
```

Ex:
```bash
python merge_peft_adapter.py --model_type auto --base_model hllj/mistral-vi-math --tokenizer_path lora --lora_model lora --output_dir final
```

## Inference

```bash
python inference_vllm.py --model_path <output folder for merged model> --max_new_tokens 1024 --temperature 0.1 --output_filepath submission.csv
```

Ex:
```bash
python inference_vllm.py --model_path final --max_new_tokens 1024 --temperature 0.1 --output_filepath submission.csv
```